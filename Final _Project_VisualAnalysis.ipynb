{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9q45wJlxCOKyuTelZjcnu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9egzsJtIYqyv","executionInfo":{"status":"ok","timestamp":1713211921022,"user_tz":240,"elapsed":18815,"user":{"displayName":"Diego Deviez","userId":"00144110995458610834"}},"outputId":"0dbaa827-5914-4689-b0b9-63dda2b53db1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os, io, cv2, base64, requests, json, time, sys\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#Converting img folder into img filenames.txt pt 1\n","import os\n","\n","folder_path='/content/drive/MyDrive/Final Project/bbcnews'\n","\n","filenames = os.listdir(folder_path)\n","print(filenames)"],"metadata":{"id":"y_BJA1SfY00f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Converting img folder into img filenames.txt pt 2\n","output_file_path = '/content/drive/MyDrive/Final Project/bbc img filenames.txt'\n","\n","with open(output_file_path, 'w') as f:\n","    for name in filenames:\n","        f.write(name + '\\n')\n","\n","print(\"Filenames have been saved to:\", output_file_path)\n"],"metadata":{"id":"L_wClcbTZCia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Converting img folder into img filenames.txt pt 3\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","print(imgnamefilepath)\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","for i, line in enumerate(imgnamefile.readlines()[:]):\n","    imgname = line.rstrip(\"\\r\\n\")\n","    print(i, imgname)"],"metadata":{"id":"ype96MfHbJBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Face ++ multiple Face Analysis\n","API_KEY = \"GFnZ6fljq23SWOe-Sq5TfswFtsxl14h0\"\n","API_SECRET = \"Zx-pEmLroe-4QEFjImmLb6MEHfebPdz9\"\n","\n","http_url = 'https://api-us.faceplusplus.com/facepp/v3/detect'\n","\n","attributes=\"gender,age,emotion\"\n","\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","print(imgnamefilepath)\n","\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","for i, line in enumerate(imgnamefile.readlines()[:]):\n","    imgname = line.rstrip(\"\\r\\n\")\n","    print(i, imgname)\n","\n","\n","    imgpath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbcnews\", imgname)\n","\n","    if len(imgpath) > 255:\n","        imgpath = \"/\".join(imgpath.split(\"/\")[1:])\n","    image_file = open(imgpath, \"rb\")\n","    encoded_string = base64.b64encode(image_file.read())\n","\n","\n","    r = requests.post(http_url,  data = {'api_key': API_KEY,\n","                             'api_secret': API_SECRET,\n","                             'image_base64': encoded_string,\n","                             'return_landmark': 0,\n","                             'return_attributes': attributes})\n","\n","    print(r.text)\n","\n","    js = json.loads(r.text)\n","    print(json.dumps(js, indent=4))\n","\n","\n","    faces = js[\"faces\"]\n","    numface = len(faces)\n","    print(\"number of faces\",numface)\n","\n","    if numface == 0:\n","\n","        wlist = [imgname, numface]\n","        print(wlist)\n","\n","        slist = [str(x) for x in wlist]\n","        print(slist)\n","\n","        savelist = \"\\t\".join(slist)\n","        resultpath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbcnews results.txt\")\n","\n","\n","        with open(resultpath, 'a') as file:\n","            file.write(savelist + '\\n')\n","\n","    for faceid in range(0, numface):\n","        if faceid > 4: continue\n","        print(\"face id\", faceid)\n","        this_face = faces[faceid]\n","        width = this_face[\"face_rectangle\"][\"width\"]; height = this_face[\"face_rectangle\"][\"height\"]\n","        top = this_face[\"face_rectangle\"][\"top\"]; left = this_face[\"face_rectangle\"][\"left\"]\n","        size = width * height\n","        print(\"face size\",size)\n","        gender = this_face[\"attributes\"][\"gender\"][\"value\"]\n","        age = this_face[\"attributes\"][\"age\"][\"value\"]\n","        anger = this_face[\"attributes\"][\"emotion\"][\"anger\"]\n","        disgust = this_face[\"attributes\"][\"emotion\"][\"disgust\"]\n","        fear = this_face[\"attributes\"][\"emotion\"][\"fear\"]\n","        happiness = this_face[\"attributes\"][\"emotion\"][\"happiness\"]\n","        neutral = this_face[\"attributes\"][\"emotion\"][\"neutral\"]\n","        sadness = this_face[\"attributes\"][\"emotion\"][\"sadness\"]\n","        surprise = this_face[\"attributes\"][\"emotion\"][\"surprise\"]\n","\n","        print(\"gender\",gender)\n","        print(\"age\",age)\n","        print(\"emotion\",anger, disgust, fear, happiness, neutral, sadness, surprise)\n","\n","\n","        wlist = [imgname, numface, faceid, gender, age, anger, disgust, fear, happiness, neutral, sadness, surprise]\n","        print(wlist)\n","\n","        slist = [str(x) for x in wlist]\n","        print(slist)\n","\n","        savelist = \"\\t\".join(slist)\n","        resultpath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbcnews results.txt\")\n","\n","        with open(resultpath, 'a') as file:\n","            file.write(savelist + '\\n')\n","\n","\n","    time.sleep(1)"],"metadata":{"id":"HQzBfedsbOYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Running an Aesthetic Analysis\n","import os, sys, cv2\n","import numpy as np\n","\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","print(imgnamefilepath)\n","\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","for i, line in enumerate(imgnamefile.readlines()[:]):\n","    imgname = line.rstrip(\"\\r\\n\")\n","    print(i, imgname)\n","\n","    # create the image path\n","    imgpath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbcnews\", imgname)\n","\n","    # get the file size\n","    filesize = os.path.getsize(imgpath)\n","    print(filesize)\n","\n","    img = cv2.imread(imgpath)\n","    h, w = img.shape[:2]\n","    ar = w/h; size = w*h\n","    dia = (w**2 + h**2)**0.5\n","    wfilesize = filesize / size\n","\n","    # convert image from BGR color model to HSV color model\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","\n","    # get average H, S, V (hue, saturation, and value, which represents brightness)\n","    hsvH, hsvS, hsvV = cv2.split(hsv)\n","    mean_hsvH = np.mean(hsvH.flatten())\n","    mean_hsvS = np.mean(hsvS.flatten())\n","    mean_hsvV = np.mean(hsvV.flatten())\n","\n","    # create a list that has all the values I want to store\n","    wlist = [imgname, filesize, h, w, ar, wfilesize, mean_hsvH, mean_hsvS, mean_hsvV]\n","    print(wlist)\n","    # transform all the elements in this list to string format\n","    slist = [str(x) for x in wlist]\n","    print(slist)\n","    # combine all the elements in this list and tranform them into a text string\n","    savelist = \"\\t\".join(slist)\n","    resultpath = os.path.join('/content/drive/MyDrive', \"Final Project\", \"bbcnews aesthetics results.txt\")\n","\n","    # open a file for writing, and save the results\n","    with open(resultpath, 'a') as file:\n","        file.write(savelist + '\\n')"],"metadata":{"id":"iSn4LjlefQ02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Img clustering\n","# download and examine the pre-trained model\n","\n","from google.colab import drive\n","import cv2, os, joblib\n","import numpy as np\n","import keras.utils as image\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","\n","# check the pre-trained model\n","base_model = VGG16(weights=\"imagenet\", include_top=True)\n","print(base_model.summary())\n","\n","# build the pre-trained model to use\n","feature_model = Model(base_model.input, base_model.get_layer('fc1').output)\n","img_size = (224, 224)\n","print(feature_model.summary())"],"metadata":{"id":"_B96IvAfkB53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature extraction with the pre-trained model for each image\n","\n","# connect to google drive\n","drive.mount('/content/drive')\n","import numpy as np\n","\n","# read .txt from google drive\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","print(imgnamefilepath)\n","\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","# enumerate each line and extract features\n","for i, line in enumerate(imgnamefile.readlines()[:]):\n","    # here I only run the first 10 files. delete 10 if you are running the entire dataset\n","    imgname = line.rstrip(\"\\r\\n\")\n","    print(i, imgname)\n","\n","    # find the path to my image\n","    imgpath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbcnews\", imgname)\n","\n","    # find how large the image file is\n","    filesize = os.path.getsize(imgpath)\n","    print(filesize)\n","\n","    # this is the folder I store all my extracted features for images\n","    exfolder = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img exfeature\", \"\")\n","\n","    # preprocess the image and extract features with this pre-trained model\n","    img = image.load_img(imgpath, target_size=img_size) # read the image\n","    image_array = image.img_to_array(img) # convert the image to a numpy array\n","\n","    image_expand = np.expand_dims(image_array, 0)\n","    x_train = preprocess_input(image_expand) # normalize the image to 0-1 range\n","    features_x = feature_model.predict(x_train) # extract features for each image\n","    print(features_x)\n","    print(np.shape(features_x)) # print out the shape of extracted features\n","\n","    # save extracted features\n","    featuresavepath = os.path.join(exfolder, imgname)\n","    joblib.dump(features_x[0], featuresavepath)\n","    print(\"--SUCCESS--\"*10)\n","\n","print(\"DONE\"*20)"],"metadata":{"id":"WrMEvpIAkHZS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# combine all the extracted features into one large file\n","\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","# combine all the extracted features into one\n","features_array = []\n","\n","exfolder = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img exfeature\", \"\")\n","\n","for lineindex, line in enumerate(imgnamefile.readlines()[:]):\n","    imgname = line.rstrip('\\r\\n')\n","    ex_feature_path = os.path.join(exfolder, imgname)\n","    imgexfeatures = joblib.load(ex_feature_path)\n","    flatten_features = np.ndarray.flatten(imgexfeatures)\n","    # print(flatten_features)\n","    features_array.append(flatten_features)\n","\n","# save the file\n","features_array = np.array(features_array)\n","print(features_array)\n","features_savepath = os.path.join(exfolder, \"features combine.dat\")\n","joblib.dump(features_array,features_savepath)\n","print(\"DONE\"*20)"],"metadata":{"id":"tHVbdXVlmaKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inspected our combined file of extracted features\n","\n","exfolder = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img exfeature\", \"\")\n","features_savepath = os.path.join(exfolder, \"features combine.dat\")\n","features_array = joblib.load(features_savepath)\n","features_array = np.array(features_array)\n","print(features_array)\n","print(np.shape(features_array))\n"],"metadata":{"id":"PIK-vObBmqmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# conduct PCA to the extracted features\n","\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","imgnamefile = open(imgnamefilepath, \"r\")\n","\n","features_savepath = os.path.join(exfolder, \"features combine.dat\")\n","features_array = joblib.load(features_savepath)\n","x = pd.DataFrame(features_array)\n","print(x)\n","\n","# standardizing the features\n","x = StandardScaler().fit_transform(x)\n","\n","# PCA\n","pca = PCA(0.95) #95% of the variance is retained.\n","pca.fit(x)\n","x_pca = pca.transform(x)\n","\n","# save PCA\n","savefolder = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img PCA\")\n","# os.makedirs(savefolder, exist_ok=True)\n","pca_savepath = os.path.join(savefolder, 'PCA.dat')\n","joblib.dump(x_pca, pca_savepath)\n","\n","# save components\n","components = pca.components_\n","savepath = os.path.join(savefolder, 'components.txt')\n","components = pd.DataFrame(components)\n","components.to_csv(savepath, header=None, index=None, sep='\\t', mode='a')\n","\n","# save variance explained ratio\n","variance = pca.explained_variance_ratio_\n","savepath = os.path.join(savefolder, 'variance.txt')\n","variance = pd.DataFrame(variance)\n","variance.to_csv(savepath, header=None, index=None, sep='\\t', mode='a')\n","\n","print(\"DONE\"*20)"],"metadata":{"id":"EdZToVrJmtou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inspected our combined file of extracted features after PCA\n","\n","features_savepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img PCA\", \"PCA.dat\")\n","features_array = joblib.load(features_savepath)\n","features_array = np.array(features_array)\n","print(features_array)\n","print(np.shape(features_array))"],"metadata":{"id":"cLl8mCfWm6uq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run kmeans clustering\n","\n","from sklearn.cluster import KMeans\n","\n","#imgnamefilepath = os.path.join('/content/drive/MyDrive',\"File4900\", \"Image clustering\", \"img all filename.txt\")\n","#imgnamefile = open(imgnamefilepath, \"r\")\n","\n","features_savepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"img PCA\", \"PCA.dat\")\n","features_array = joblib.load(features_savepath)\n","X = pd.DataFrame(features_array)\n","print(X)\n","\n","# we cluster on the first 200 components\n","nd = 200\n","X = X.iloc[:,0:nd]\n","\n","# run k-means varying the number of clusters from 5 to 11\n","for K in range(5, 11):\n","    print('number of cluster', K)\n","    cl = KMeans(K, random_state=0)\n","    cl.fit(X)\n","    labels = cl.labels_\n","\n","    # merge the k-means results with image names\n","    imgnamefilepath = os.path.join('/content/drive/MyDrive',\"Final Project\", \"bbc img filenames.txt\")\n","    df = pd.read_csv(imgnamefilepath, sep ='\\t', header = None)\n","    df.columns = ['imgname']\n","    df['label'] = labels\n","    print(df)\n","\n","    # save the labels\n","    filepath = os.path.join('/content/drive/MyDrive', 'Final Project', 'image clustering', 'img cluster', str(K))\n","    os.makedirs(filepath, exist_ok=True)\n","    filepath = os.path.join(filepath, 'label.csv')\n","    df.to_csv(filepath, index = None, header = None, sep = '\\t')\n","    print(\"--SUCCESS--\"*10)\n","\n","print(\"DONE\"*50)"],"metadata":{"id":"AZpCPz5nnMf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# randomly select images from each cluster so we can inspect the theme of each cluster\n","\n","import random, shutil\n","import pandas as pd\n","\n","# this function copy one image to another folder\n","def save_img(random_id, imgname, label, K):\n","    imgpath1 = os.path.join('/content/drive/MyDrive', 'Final Project', 'bbcnews',imgname)\n","    imgfolder = os.path.join('/content/drive/MyDrive', 'Final Project', 'image clustering', 'img cluster', str(K), str(label), \"\")\n","    os.makedirs(imgfolder, exist_ok=True)\n","    newimgname = str(random_id)+' '+imgname\n","    imgpath2 = os.path.join(imgfolder, newimgname)\n","    shutil.copy(imgpath1, imgpath2)\n","\n","for K in range(5, 11):\n","    print('number of cluster',K)\n","\n","    filepath = os.path.join('/content/drive/MyDrive', 'Final Project', 'image clustering', 'img cluster', str(K), 'label.csv')\n","    df = pd.read_csv(filepath, sep ='\\t', header = None)\n","    df.columns = ['imgname',\"label\"]\n","\n","    dr = df.sample(frac=1, random_state=42) # random shuffle images\n","    dr['random_id'] = np.arange(len(dr)) # add random id\n","\n","    for labelK in range(0, K):\n","        dc = dr.loc[dr['label'] == labelK] # select images from each cluster\n","        dc = dc.iloc[:20,:] # select 20 images from each cluster\n","        dc.apply(lambda row: save_img(row['random_id'],row['imgname'],row['label'], K),axis=1)\n","\n","    print(\"--SUCESS--\"*10)\n","print(\"DONE\"*20)"],"metadata":{"id":"W3dAfaHonsig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize each cluster in a large image grid\n","\n","import glob\n","from PIL import Image, ImageDraw, ImageFont\n","from natsort import natsorted\n","\n","def fill_square(im, tbw):\n","    size = (tbw, tbw)\n","    bg = Image.new('RGB', size, \"white\")  # create a background image\n","    im.thumbnail(size, Image.ANTIALIAS)\n","    w, h = im.size\n","    bg.paste(im, (int((size[0]-w)/2), int((size[1]-h)/2)))\n","    return(bg)\n","\n","w_sq = 300 # width/height of image\n","w_clusterid = 60 # width of text for cluster id\n","h_imgid = 60 # height of gap\n","# fntcluster = ImageFont.truetype('/Library/Fonts/Arial.ttf', 60)\n","\n","for K in range(5, 11):\n","    ncol = 20 # how many images to show in each row\n","    nrow = 1\n","    nimg = ncol * nrow # total number of image in each cluster to show\n","    large_w = w_sq*ncol + w_clusterid # total width of the entire image\n","    large_h = (w_sq + h_imgid) * nrow * K # total height of the entire image\n","    large = Image.new('RGB', (large_w, large_h), \"white\")\n","\n","    for label in range(0, K):\n","        print('-'* 10, K, label)\n","        imgpathfolder = os.path.join('/content/drive/MyDrive', 'Final Project', 'image clustering', 'img cluster', str(K), str(label),'')\n","        imgfiles = glob.glob(imgpathfolder + \"*.jpg\")\n","        imgfiles = natsorted(imgfiles)[:nimg]\n","\n","        d = ImageDraw.Draw(large)\n","        d.text((0, label * nrow * (w_sq + h_imgid) + 15), str(label+1), fill=(0, 0, 0))\n","\n","        for j, imgfile in enumerate(imgfiles):\n","            jw = j%ncol; jh = j//ncol\n","            img = Image.open(imgfile)\n","            im_sq = fill_square(img, w_sq)\n","            im_sq_x = jw*w_sq + w_clusterid\n","            im_sq_y = h_imgid + jh* (w_sq + h_imgid) + label * nrow * (w_sq + h_imgid)\n","            large.paste(im_sq, (im_sq_x, im_sq_y))\n","\n","    large_savepath = os.path.join('/content/drive/MyDrive', 'Final Project', 'img grid', str(K)+\".png\")\n","    large.save(large_savepath)\n","\n","print(\"DONE\"*20)"],"metadata":{"id":"2wzzMgtAoFE4"},"execution_count":null,"outputs":[]}]}